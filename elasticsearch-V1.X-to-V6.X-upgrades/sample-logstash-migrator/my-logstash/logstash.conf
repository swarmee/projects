input {
  elasticsearch {
    hosts => "${SOURCE_ELASTICHOST:localhost}"
    index => "${SOURCE_INDEX:localhost}"
    query => '{ "query": { "match_all" : {}  }}'  # the query can be customised to pull just a small chuck of data for testing purposes.
    size => 10                                    # this is just for testing - change to 10,000 for production migration
    scroll => "5m"                                # 5 minutes should be plenty - we should be looking at less than 10 seconds for each 10k batch
    docinfo => true                               # this allows the metadata from the old index to be carried across to the new index.
    user => "${SOURCE_USERNAME:elastic}"
    password => "${SOURCE_PASSWORD:changeme}"
    ssl => "true"
  }
}

filter {
#  date {
#    match => [ "updated_at", "YYYY-MM-dd HH:mm:ss" ]
#  }
  mutate {
    remove_field => ["@timestamp", "@version"]
  }

}

output {

##stdout { codec => rubydebug }                      # if you want to see output for testing purposes

  elasticsearch {
    hosts => ["${TARGET_ELASTICHOST:localhost}"]
    user => "${TARGET_USERNAME:elastic}"
    password => "${TARGET_PASSWORD:changeme}"
    ssl => "true"
    action => "update"                               # we are upserting so we can run the migration process mulitple times into the same index if required
    doc_as_upsert => true
    index => "%{[@metadata][_index]}"
    document_type => "_doc"                          # set to _doc for all new indexes
    document_id => "%{[@metadata][_id]}"
    template      => "/usr/share/logstash/config/mapping.json"
    template_name => "${TEMPLATE_NAME:my-index}"
    template_overwrite => true
    }
}
