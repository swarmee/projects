input {
    file {        path => "/tmp/geonames/data/cities1000.txt"       
#                  start_position => beginning
#                  sincedb_path => "/dev/null"
#                  ignore_older => 0 
         }
}

filter {
  csv {
     skip_empty_columns => true
     separator => "	"
     columns => [
                 "geonameId"                 
                 ,"name"
                 ,"asciiName"
                 ,"alternateNames"
                 ,"latitude"
                 ,"longitude"
                 ,"featureClass"
                 ,"featureCode"
                 ,"countryCode"
                 ,"altCountryCode"
                 ,"admin1Code"
                 ,"admin2Code"
                 ,"admin3Code"
                 ,"admin4Code"
                 ,"population" 
                 ,"elevation"
                 ,"digitalElevationModel"
                 ,"timezone"
                 ,"modificationDate"
] 
  }

  date { 
    match => [ "modificationDate", "yyyy-MM-dd" ] 
    target => "modificationDate" 
       }      

  mutate {
    remove_field => [ "message", "host", "@version", "path", "@timestamp" ]
    convert => { "latitude"  => "float" }
    convert => { "longitude" => "float" }
    rename =>  { "longitude" => "[location][lon]"}
    rename =>  { "latitude"  => "[location][lat]"}
          }

if [alternateNames]
{
  mutate {
    split => { "alternateNames" => "," }
        }
}
}

output {

##     stdout { codec => rubydebug }
##     stdout { codec => dots }

  elasticsearch {
    index => "city"
    hosts => ["${ELASTICSEARCH_HOST:elasticsearch}:${ELASTICSEARCH_PORT:9200}"]
    document_type => "city"
    retry_on_conflict => 9
    action => "update"
    document_id => "%{geonameId}"
    doc_as_upsert => true
    template      => "/usr/share/logstash/config/mapping.json"
    template_name => "city"
    template_overwrite => true
    sniffing => true
    sniffing_delay => 999
}
}



 
